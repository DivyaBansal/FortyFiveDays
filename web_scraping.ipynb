{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create selenium session\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument('<user profile>')\n",
    "driver = webdriver.Chrome(executable_path='<path to chrome driver>', chrome_options=options)\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(10)\n",
    "driver.set_script_timeout(10000000)\n",
    "\n",
    "# or do this everytime\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accept cookies\n",
    "driver.get(url)\n",
    "driver.find_element(\"xpath\",'//*[@id=\"cmpwelcomebtnyes\"]/a').click()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinite scrolling\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button in pop-up to go to the site\n",
    "url = 'https://www.autoscout24.de/detailsuche?sort=standard&desc=0&ustate=N,U&atype=C&cy=D&ocs_listing=include'\n",
    "driver.get(url)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "wait.until(EC.frame_to_be_available_and_switch_to_it((By.ID, \"gdpr-consent-notice\")))\n",
    "button = driver.find_element('id','save')\n",
    "driver.execute_script(\"arguments[0].click();\", button)\n",
    "driver.switch_to.parent_frame()\n",
    "driver.find_element('xpath','//*[@id=\"__next\"]/div/div/div[2]/div[2]/div[1]/div/div[1]/div[2]/div[1]/div/div[1]/div/div[1]/div/button').click()\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '....'\n",
    "driver.get(url)\n",
    "\n",
    "# Beautifyul Soup example\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "product_name = soup.find('h1', class_='sale-box__product-name').get_text().strip()\n",
    "price_box = soup.find('div', class_='sale-box__price-wrapper')\n",
    "\n",
    "if price_box.find('span', class_='sale-box__price sale-box__price--base'):\n",
    "    price = price_box.find('span', class_='sale-box__price sale-box__price--base').get_text().replace('â‚¬', '').strip()\n",
    "\n",
    "# if you want to check for attribute\n",
    "name = article.find('h2').get_text()\n",
    "if article.has_attr('data-lazyloaddata'):\n",
    "    try:\n",
    "        lazy = json.loads(article.get('data-lazyloaddata'))\n",
    "        if \"branche\" in lazy:\n",
    "            branch = lazy[\"branche\"]\n",
    "        if \"adresseKompakt\" in lazy:\n",
    "            address_fields = lazy[\"adresseKompakt\"]\n",
    "            if 'stadtteil' in address_fields:\n",
    "                district = address_fields[\"stadtteil\"]                            \n",
    "            if \"telefonnummer\" in address_fields:\n",
    "                telefonnummer = address_fields[\"telefonnummer\"]\n",
    "            if \"plzOrt\" in address_fields:\n",
    "                plzOrt = address_fields[\"plzOrt\"]\n",
    "                zip_code = re.findall(r'\\d+',plzOrt)\n",
    "                if zip_code:\n",
    "                    zip_code = zip_code[0]\n",
    "                else:\n",
    "                    zip_code = '' \n",
    "                city = plzOrt.replace(zip_code,'').strip()\n",
    "            if \"strasseHausnummer\" in address_fields:\n",
    "                street = address_fields[\"strasseHausnummer\"]\n",
    "            address = street + ' ' + plzOrt + ' ' + district\n",
    "\n",
    "# attribute condition\n",
    "addr = article.find('address').find('p',{'data-wipe-name':'Adresse'})\n",
    "\n",
    "# load more button (click in loop) like\n",
    "loadmore = driver.find_element('id',\"mod-LoadMore--button\")\n",
    "try:\n",
    "    while loadmore.is_displayed():\n",
    "        loadmore.click()\n",
    "        time.sleep(6)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        articles = soup.find_all('article', class_='mod mod-Treffer')\n",
    "        print(len(articles))\n",
    "        with open('articles.txt', 'wb') as fp:\n",
    "            pickle.dump(str(soup), fp)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# Scraping a table\n",
    "table1 = soup.find('table')\n",
    "\n",
    "headers = []\n",
    "for i in table1.find_all('th'):\n",
    "    title = i.text.strip()\n",
    "    headers.append(title)\n",
    "\n",
    "mydata = pd.DataFrame(columns = headers)\n",
    "for j in table1.find_all('tr')[1:]:\n",
    "    row_data = j.find_all('td')\n",
    "    if row_data:\n",
    "        row = [i.text.strip() for i in row_data]\n",
    "        length = len(mydata)\n",
    "        mydata.loc[length] = row\n",
    "mydata.to_excel(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium example\n",
    "filter_box = Select(driver.find_element('xpath', '//*[@id=\"selectProv-33c9327b-8dd7-11ea-b27f-005056a48f82-00055___\"]'))\n",
    "for opt in filter_box.options:\n",
    "\tprovince = opt.text\n",
    "\tfilter_box.select_by_visible_text(province)\n",
    "\ttime.sleep(1)\n",
    "# wait for element to be clickable\n",
    "\tWebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"centercontainer\"]/div[2]/div[1]/div[1]/div[2]/div/div/div[2]/div/div/div[1]/a'))).click()\n",
    "\tcards = driver.find_element('xpath', '//*[@id=\"mapAccordion_33c9327b-8dd7-11ea-b27f-005056a48f82-00055____listado\"]').find_elements(By.CLASS_NAME, 'card')\n",
    "\tfor card in cards:\n",
    "\t\tcard_btn = card.find_element(By.TAG_NAME, 'h3').find_element(By.TAG_NAME, 'button')\n",
    "\t\tname = card_btn.text\n",
    "\t\tcard_btn.click()\n",
    "\t\tcard_body = card.find_element(By.CSS_SELECTOR, 'div.card-body.bg-traffic').find_elements(By.CSS_SELECTOR, 'div.col-12.col-md-6.text-to-resize')\n",
    "\t\tcontact = card_body[0].find_element(By.CSS_SELECTOR, 'div.col-12.col-md-9.text-to-resize').find_elements(By.CLASS_NAME, 'fs-14')\n",
    "\t\tfor c in contact:\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\t\ttry:\n",
    "\t\t\t\tx = c.find_element(By.TAG_NAME, 'a').text.strip()\n",
    "\t\t\t\tif x and 'null' not in x:\n",
    "\t\t\t\t\tif '@' in x:\n",
    "\t\t\t\t\t\temail = x\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tphone = x\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\taddress = card_body[1].find_element(By.CSS_SELECTOR, 'div.col-12.col-md-9.text-to-resize').find_elements(By.CLASS_NAME, 'fs-14')\n",
    "\t\tstreet = address[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium example select options then submit and scrape resulting page\n",
    "\n",
    "for combos in make_model_combinations:\n",
    "    hersteller, modell, motor = combos\n",
    "    driver.get(url)\n",
    "    select_hersteller = Select(driver.find_element('xpath', '//*[@id=\"stepbystep0\"]'))\n",
    "    select_hersteller.select_by_visible_text(hersteller)\n",
    "    time.sleep(0.5)\n",
    "    select_modell = Select(driver.find_element('xpath', '//*[@id=\"stepbystep1\"]'))\n",
    "    select_modell.select_by_visible_text(modell)\n",
    "    time.sleep(0.5)\n",
    "    select_motor = Select(driver.find_element('xpath', '//*[@id=\"stepbystep2\"]'))\n",
    "    select_motor.select_by_visible_text(motor)\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element('xpath','//*[@id=\"vehiclecontext_stepbystep\"]/div[4]/button').click()\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    products = soup.find_all('li', class_='item-product')\n",
    "    for product in products:\n",
    "        title = product.find('div', class_='s1 title').get_text().strip()\n",
    "        note_price = product.find('span', class_='note-price').get_text().strip()\n",
    "        price_big = product.find('span', class_='price-big').get_text().strip()\n",
    "        items.append([hersteller, modell, motor, title, note_price, price_big])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter value in text box example\n",
    "\n",
    "input = driver.find_element('xpath','//*[@id=\"make-input-primary-filter\"]')\n",
    "input.send_keys(make)\n",
    "input.send_keys(Keys.ENTER)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Scraper example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "url = '...'\n",
    "html_home_page = scraper.get(url).text\n",
    "soup = BeautifulSoup(html_home_page, 'html.parser')\n",
    "articles = soup.find_all('div', class_='...')\n",
    "for article in articles:\n",
    "    detail_site = article.get('onclick').replace('location.href=','')[1:-1]\n",
    "    html_article = scraper.get(detail_site).text\n",
    "    article_soup = BeautifulSoup(html_article, 'html.parser')\n",
    "    # and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode encoded string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def cfDecodeEmail(encodedString):\n",
    "    r = int(encodedString[:2],16)\n",
    "    email = ''.join([chr(int(encodedString[i:i+2], 16) ^ r) for i in range(2, len(encodedString), 2)])\n",
    "    return email\n",
    "\n",
    "email = article_soup.find('div', id= 'cp-email').find('span', class_='__cf_email__').get('data-cfemail')\n",
    "email = cfDecodeEmail(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sleeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "# Sleep for random (1-5) seconds\n",
    "sleep(randint(1,5))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
