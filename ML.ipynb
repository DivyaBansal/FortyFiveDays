{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "standardizer = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#  Balancing the classes using SMOTE\n",
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "x_train_res, y_train_res = sm.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn pipelines\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_pipe = make_pipeline(SimpleImputer(), MinMaxScaler())\n",
    "cat_pipe = make_pipeline(SimpleImputer(\"most_frequent\"), \n",
    "                         OneHotEncoder())\n",
    "\n",
    "processor = make_column_transformer(\n",
    "    (num_pipe, [\"num_feats\"]), \n",
    "    (cat_pipe, [\"cat_feats\"])\n",
    ")\n",
    "\n",
    "log_pipe= make_pipeline(processor, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To HTML display pipeline \n",
    "from sklearn import set_config\n",
    "set_config(display = 'diagram')\n",
    "\n",
    "# ...define pipeline...\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(df)\n",
    "X_test = ohe.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoder for e.g. good, bad, worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder()\n",
    "diamonds.loc[:, cats] = oe.fit_transform(diamonds[cats])  # cats is a list of the names of all categorical columns that need to be ordinally encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dte = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latent topic categories from strings\n",
    "\n",
    "GapEncoder\n",
    "\n",
    "MinHashEncoder -> fast, stateless (for distributed computing), not interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Drop column\n",
    "df = df.drop(columns=\"column_name\")\n",
    "\n",
    "# Remove non-alphabetic text from column\n",
    "df['text_col'] = df['text_col'].str.replace('[^a-zA-Z]','')\n",
    "\n",
    "#  Replace nulls\n",
    "df = df.fillna('')\n",
    "\n",
    "df = df.dropna(subset=\"name\")\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "df['x'] = ss.fit_transform(df[['x']])\n",
    "df['x'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer()\n",
    "df['price'] = pt.fit_transform(df[['price']])\n",
    "df['price'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputations\n",
    "\n",
    "- KNNImputer - classic KNN but for missing data. Data points are imputed by averaging the value of its n-neighbors or by taking their mode if categorical.\n",
    "\n",
    "- IterativeImputer - accepts any model as an estimater and models missing values as a function existing features. In other words, training data becomes all the rows without missing data and the test set is all those that are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Copy the data\n",
    "knn_imputed = diabetes.copy(deep=True)\n",
    "# Init the transformer\n",
    "imp = KNNImputer(n_neighbors=3)\n",
    "# Fit/transform\n",
    "knn_imputed.loc[:, :] = imp.fit_transform(knn_imputed)\n",
    "\n",
    "#  or ...\n",
    "\n",
    "#  Init\n",
    "imp = IterativeImputer(\n",
    "    estimator=ExtraTreesRegressor(), \n",
    "    max_iter=10\n",
    ")\n",
    "\n",
    "# Tranform\n",
    "ii_imputed.loc[:, :] = imp.fit_transform(ii_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the effectiveness of a data imputation technique by plotting KDE plots\n",
    "\n",
    "The closer they are, the more similar the imputed points are to the real distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original distribution\n",
    "sns.kdeplot(diabetes.SkinThickness, label=\"Original Distribution\")\n",
    "\n",
    "for k in n_neighbors:\n",
    "    knn_imp = KNNImputer(k)\n",
    "    diabetes.loc[:, :] = knn_imp.fit_transform(diabetes)\n",
    "    # Plot again once imputed    \n",
    "    sns.kdeplot(diabetes.SkinThickness, label=f\"Imputed Dist with k={k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "tsne = TSNE(n_components=2)\n",
    "manifold = umap.UMAP(n_components=2)\n",
    "\n",
    "X_transformed = pca.fit_transform(X)\n",
    "X_transformed = tsne.fit_transform(X)\n",
    "X_transformed = manifold.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "_ = clf.fit(X, y)\n",
    "\n",
    "# Printing prediction path\n",
    "text = tree.export_text(clf)\n",
    "print(text)\n",
    "\n",
    "# Visualizing tree \n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "_ = tree.plot_tree(\n",
    "    clf, feature_names=iris.feature_names, \n",
    "    class_names=iris.target_names, filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    ">>> metrics.get_scorer(\"roc_auc\")\n",
    "make_scorer(roc_auc_score, needs_threshold=True)\n",
    "\n",
    ">>> metrics.get_scorer(\"precision\")\n",
    "make_scorer(precision_score, average=binary)\n",
    "\n",
    ">>> metrics.get_scorer(\"r2\")\n",
    "make_scorer(r2_score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
